{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HojatqAscRS4"
   },
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sGhvgzGhcpho"
   },
   "outputs": [],
   "source": [
    "# importing libaries\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "import gzip\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gYXAZa7bcHhQ"
   },
   "outputs": [],
   "source": [
    "# The following code was copied from the textbook:\n",
    "# Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
    "# I modified the head of the model to make it a predict\n",
    "# the amplitude of the audio signal that immediately follows\n",
    "# the 5 amplitudes that precede it (the input)\n",
    "wavenet_model = tf.keras.Sequential()\n",
    "wavenet_model.add(tf.keras.layers.Input(shape=[None, 5]))\n",
    "for rate in (1, 2, 4, 8) * 2:\n",
    "  wavenet_model.add(tf.keras.layers.Conv1D(\n",
    "  filters=32, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=rate))\n",
    "wavenet_model.add(tf.keras.layers.Conv1D(filters=1, kernel_size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2pdA8lJXRGt"
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHUnZqAWXXXA"
   },
   "source": [
    "I downloaded train_normal.tar.gz, test_normal.tar.gz, and test_anomaly.tar.gz, placing them into my working directory. The following are useful functions for processing these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4GRM61eSPJlc"
   },
   "outputs": [],
   "source": [
    "# extracts file, placing it into directory with same name\n",
    "# as file without the file extension\n",
    "def extract(file):\n",
    "  with gzip.open(file, 'rb') as inFile:\n",
    "      with tarfile.open(fileobj=inFile, mode='r') as tar:\n",
    "          tar.extractall(path=file.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qHRLg1jpUYFE"
   },
   "outputs": [],
   "source": [
    "# recursively deletes a directory\n",
    "def delete(directory):\n",
    "  shutil.rmtree(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mEAAkRRuXAus"
   },
   "outputs": [],
   "source": [
    "# returns list of path for all files\n",
    "# in a directory\n",
    "def paths(directory):\n",
    "  result = []\n",
    "  for fileName in os.listdir(directory):\n",
    "    result.append(directory + \"/\" + fileName)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntHY3oGCa6FL"
   },
   "source": [
    "The following function generates our data from a given file. It will include the amplitude for 5 frames with the 6th frame being the output or label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "z5eUPeMaY4jG"
   },
   "outputs": [],
   "source": [
    "# returns list of training data for\n",
    "# a file\n",
    "def generate(file_path):\n",
    "  audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "  audio_data = librosa.util.normalize(audio_data)\n",
    "  audio_data = audio_data.flatten()\n",
    "  data = []\n",
    "  labels = []\n",
    "  for i in range(0, len(audio_data) - 3, 6):\n",
    "    data.append(audio_data[i:i+5])\n",
    "    labels.append(audio_data[i+5])\n",
    "  return (np.array(data), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9XeEIX0zUV3o"
   },
   "outputs": [],
   "source": [
    "extract(\"train_normal.tar.gz\")\n",
    "extract(\"test_normal.tar.gz\")\n",
    "extract(\"test_anomaly.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dre6fo75dlWc"
   },
   "outputs": [],
   "source": [
    "train_files = paths(\"train_normal\")\n",
    "test_files = paths(\"test_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quartering the amount of training data\n",
    "# due to limited RAM capabilities\n",
    "train_files = train_files[:len(train_files)//4]\n",
    "test_files = test_files[:len(test_files)//4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "6h8rVkWqdaPI"
   },
   "outputs": [],
   "source": [
    "# generates training data/labels\n",
    "x_train = []\n",
    "y_train = []\n",
    "for f in train_files:\n",
    "  res = generate(f)\n",
    "  x_train.append(res[0])\n",
    "  y_train.append(res[1])\n",
    "temp = []\n",
    "for x in x_train:\n",
    "    for y in x:\n",
    "        temp.append(y)\n",
    "x_train = np.array(temp)\n",
    "y_train = np.array(y_train).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "pWcg9rF4gRoV"
   },
   "outputs": [],
   "source": [
    "# generates testing data/labels\n",
    "x_test = []\n",
    "y_test = []\n",
    "for f in test_files:\n",
    "  res = generate(f)\n",
    "  x_test.append(res[0])\n",
    "  y_test.append(res[1])\n",
    "temp = []\n",
    "for x in x_test:\n",
    "    for y in x:\n",
    "        temp.append(y)\n",
    "x_test = np.array(temp)\n",
    "y_test = np.array(y_test).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYuBJhSRfLxG"
   },
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Hyc04Sz4frQ5"
   },
   "outputs": [],
   "source": [
    "wavenet_model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [69], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_train[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_test))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(len(x_train[0]))\n",
    "print(len(y_train))\n",
    "print(len(x_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "#x_train = np.reshape(x_train, (batch_size, 1, 5))\n",
    "#x_test = np.reshape(x_test, (batch_size, 1, 5))\n",
    "#y_train = np.reshape(y_train, (batch_size, 1, 1))\n",
    "#y_test = np.reshape(y_test, (batch_size, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "UL7GP2dzhPOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"conv1d\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 5)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 5), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwavenet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file8kjsgong.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\saumy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"conv1d\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 5)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 5), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "wavenet_model.fit(x_train, y_train, batch_size=batch_size, epochs=10, validation_data=(x_test, y_test), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
